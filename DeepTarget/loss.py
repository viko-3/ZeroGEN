import mathimport torchimport torch.nn as nnimport torch.nn.functional as Fdef cross_attention_loss():    passclass SupConLossPLMS(nn.Module):    """Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning: https://arxiv.org/abs/2011.01403    """    def __init__(self, device, temperature=0.05):        super(SupConLossPLMS, self).__init__()        self.tem = temperature        self.device = device    def forward(self, batch_emb, labels=None):        labels = labels.view(-1, 1)        batch_size = batch_emb.shape[0]        mask = torch.eq(labels, labels.T).float()        norm_emb = F.normalize(batch_emb, dim=1, p=2)        # compute logits        dot_contrast = torch.div(torch.matmul(norm_emb, norm_emb.T), self.tem)        # for numerical stability        logits_max, _ = torch.max(dot_contrast, dim=1, keepdim=True)  # _返回索引        logits = dot_contrast - logits_max.detach()        # 索引应该保证设备相同        logits_mask = torch.scatter(torch.ones_like(mask), 1, torch.arange(batch_size).view(-1, 1).to(self.device), 0)        mask = mask * logits_mask        # compute log_prob        exp_logits = torch.exp(logits) * logits_mask        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))        mask_sum = mask.sum(1)        # 防止出现NAN        mask_sum = torch.where(mask_sum == 0, torch.ones_like(mask_sum), mask_sum)        mean_log_prob_pos = -(mask * log_prob).sum(1) / mask_sum        return mean_log_prob_pos.mean()if __name__ == '__main__':    device = 'cpu'    labels = torch.tensor([1, 2, 0, 1, 1])    feature = torch.ones(5, 3)    supConLossPLMS = SupConLossPLMS(device)    res = supConLossPLMS(feature, labels)