from abc import ABC, abstractmethodimport torchimport torch.nn as nnimport torch.optim as optimfrom torch.utils.data import DataLoader, Datasetfrom tqdm.auto import tqdmimport torch.nn.functional as Fimport numpy as npfrom DeepTarget.utils import set_torch_seed_to_all_gens, WordVocab, Logger, mol_random_mask, TAPETokenizer, IUPAC_VOCAB, \    prot_random_maskfrom torch.nn.utils.rnn import pad_sequenceclass BaseDataset(Dataset):    def __init__(self, data):        self.data = data    def __getitem__(self, index):        return self.data[index]    def __len__(self):        return len(self.data)class BaseTrainer(ABC):    @property    def n_workers(self):        n_workers = self.config.n_workers        return n_workers if n_workers != 1 else 0    def get_collate_device(self, model):        n_workers = self.n_workers        return 'cpu' if n_workers > 0 else model.device    def get_dataloader(self, model, data, collate_fn=None, shuffle=True):        if collate_fn is None:            collate_fn = self.get_collate_fn(model)        return DataLoader(BaseDataset(self.prepare_data(data)),                          batch_size=self.config.n_batch,                          shuffle=shuffle,                          drop_last=False,                          num_workers=self.n_workers, collate_fn=collate_fn,                          worker_init_fn=set_torch_seed_to_all_gens                          if self.n_workers > 0 else None)    def prepare_data(self, data):        mols, prots, mols_idx, prots_idx = data[0], data[1], data[2], data[3]        data = [{'mol': smiles, 'prot': proteins, 'mol_idx': mol_idx, 'prot_idx': prot_idx} for                smiles, proteins, mol_idx, prot_idx in                zip(mols, prots, mols_idx, prots_idx)]        return data    def get_collate_fn(self, model):        return None    @abstractmethod    def get_vocabulary(self, mol, protein):        pass    @abstractmethod    def fit(self, model, train_data, val_data=None):        passclass DeepTargetTrainer(BaseTrainer):    def __init__(self, config):        self.config = config        self.temp = nn.Parameter(0.07 * torch.ones([]))    def get_loop_data(self, source, i):        data = source[:, :i]        return data    def _train_epoch(self, model, tqdm_data, optimizer=None, epoch=0):        if optimizer is None:            model.eval()        else:            model.train()        postfix = {'loss': 0,                   'running_loss': 0}        for i, (mol_mask, mol_target, mol_lens, prot_input_ids, prot_input_mask, prot_labels, mol_idx, pro_idx) in enumerate(                tqdm_data):            mol_mask, mol_target = mol_mask.to(model.device), mol_target.to(model.device)            prot_input_ids, prot_input_mask = prot_input_ids.to(model.device), prot_input_mask.to(model.device)            loss = model(mol_mask, mol_target, prot_input_ids, prot_input_mask, mol_idx, pro_idx, epoch)            # ###============== all loss ===================###            if optimizer is not None:                optimizer.zero_grad()                loss.backward()                optimizer.step()            postfix['loss'] = loss.item()            postfix['running_loss'] += (loss.item() -                                        postfix['running_loss']) / (i + 1)            tqdm_data.set_postfix(postfix)        postfix['mode'] = 'Eval' if optimizer is None else 'Train'        return postfix    def _train(self, model, train_loader, val_loader=None, logger=None):        def get_params():            return (p for p in model.parameters() if p.requires_grad)        device = model.device        optimizer = optim.Adam(get_params(), lr=self.config.lr)        scheduler = optim.lr_scheduler.StepLR(optimizer,                                              self.config.step_size,                                              self.config.gamma)        model.zero_grad()        for epoch in range(self.config.train_epochs):            scheduler.step()            tqdm_data = tqdm(train_loader,                             desc='Training (epoch #{})'.format(epoch))            postfix = self._train_epoch(model, tqdm_data, optimizer, epoch)            if logger is not None:                logger.append(postfix)                logger.save(self.config.log_file)            if val_loader is not None:                tqdm_data = tqdm(val_loader,                                 desc='Validation (epoch #{})'.format(epoch))                postfix = self._train_epoch(model, tqdm_data)                if logger is not None:                    logger.append(postfix)                    logger.save(self.config.log_file)            if (self.config.model_save is not None) and \                    (epoch % self.config.save_frequency == 0):                model = model.to('cpu')                torch.save(                    model.state_dict(),                    self.config.model_save[:-3] + '_{0:03d}.pt'.format(epoch)                )                model = model.to(device)    def get_vocabulary(self, mol, protein):        mol_vocab = WordVocab.from_data(mol)        prot_vocab = IUPAC_VOCAB        return mol_vocab, prot_vocab    def get_collate_fn(self, model):        device = self.get_collate_device(model)        def collate_bert(data):            """            :param data: n_batch 个data            n x {'mol':mol, 'prot':prot, 'mol_idx':mol_idx, 'prot_idx':prot_index}            :return:            """            # 处理分子            mol_tensors = [model.mol_model.string2tensor(string['mol'], device=device)                           for string in data]            pad = model.mol_model.vocabulary.pad            res = [mol_random_mask(t, model.mol_model.vocabulary.mask) for t in mol_tensors]            mol_mask = pad_sequence([t['mask'] for t in res],                                    batch_first=True, padding_value=pad)            mol_target = pad_sequence([t['real'] for t in res],                                      batch_first=True, padding_value=pad)            mol_lens = torch.tensor([len(t) - 1 for t in mol_tensors],                                    dtype=torch.long, device=device)            # 处理蛋白            proteins = [string['prot'] for string in data]            proteins_tensor = [torch.tensor(model.prot_tokenizer.encode(protein)) for protein in proteins]            res = [prot_random_mask(protein_tensor, IUPAC_VOCAB) for protein_tensor in proteins_tensor]            prot_input_ids, prot_input_mask, prot_labels = [], [], []            for r in res:                prot_input_ids.append(r['input_ids'])                prot_input_mask.append(r['input_mask'])                prot_labels.append(r['labels'])            prot_input_ids = pad_sequence(prot_input_ids, batch_first=True, padding_value=0).long()            prot_input_mask = pad_sequence(prot_input_mask, batch_first=True, padding_value=0).long()            # ignore_index is -1            prot_labels = pad_sequence(prot_labels, batch_first=True, padding_value=-1).long()            # 处理分子idx            mol_idx = [string['mol_idx'] for string in data]            mol_idx = torch.Tensor(mol_idx)            # 处理蛋白idx            pro_idx = [string['prot_idx'] for string in data]            pro_idx = torch.Tensor(pro_idx)            return mol_mask, mol_target, mol_lens, prot_input_ids, prot_input_mask, prot_labels, mol_idx, pro_idx        return collate_bert    def fit(self, model, train_data, val_data=None):        logger = Logger() if self.config.log_file is not None else None        train_loader = self.get_dataloader(model, train_data, shuffle=True)        val_loader = None if val_data is None else self.get_dataloader(            model, val_data, shuffle=False        )        self._train(model, train_loader, val_loader, logger)        return model